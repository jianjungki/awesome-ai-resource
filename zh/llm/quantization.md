
- **后训练量化 (PTQ) 变体**：在训练后直接量化，无需重新训练；简单但可能损失精度。
- **量化感知训练 (QAT) 相关**：训练过程中模拟量化；精度更高但资源密集。
- **权重仅量化 (WOQ) 及扩展**：重点量化权重，激活保持高精度；适合推理优化。
- **非均匀/结构化量化**：使用聚类或块级处理，非线性映射；处理分布不均。
- **高级/新兴方法**：结合优化或特殊机制；常针对极低位宽或特定问题。

表格列出了每个方法的描述、优势、缺点/挑战。数据来源于常见研究和实践（如arXiv、ACL论文），我选择了最具代表性的方法以避免冗长。如果你需要添加更多细节或调整分类，让我知道！

| 分类组 | 方法 | 描述 | 优势 | 缺点/挑战 |
|--------|------|------|------|-----------|
| **后训练量化 (PTQ) 变体** | Post-Training Quantization (PTQ) | 在模型训练完成后，直接对权重和激活进行静态或动态量化，使用校准数据最小化误差。 | 简单快速，无需重新训练；显著减少内存（e.g., FP32到INT8减4倍）；易集成到框架如PyTorch。 | 可能导致精度损失，尤其是激活值；需要校准数据集优化。 |
| **后训练量化 (PTQ) 变体** | Post-Training Dynamic Quantization (PTDQ) | PTQ的动态版本，推理时实时计算量化参数（如激活范围）。 | 适应输入变化，提升鲁棒性；无需预设校准；适合实时应用。 | 推理时额外计算开销；不如静态PTQ高效。 |
| **后训练量化 (PTQ) 变体** | GPTQ (General Pre-trained Transformer Quantization) | 针对Transformer的PTQ，使用逐层Hessian近似最小化误差，支持INT3/INT4。 | 压缩率高，对大模型有效；无需大量数据；AutoGPTQ库易用。 | 计算密集，需要GPU；对异常值敏感。 |
| **后训练量化 (PTQ) 变体** | AWQ (Activation-aware Weight Quantization) | 考虑激活分布量化权重，使用通道级缩放避免误差。 | 在低位宽下精度好；少量校准数据即可；适用于LLM推理。 | 需要激活统计；实现稍复杂。 |
| **后训练量化 (PTQ) 变体** | SmoothQuant | 通过平滑激活分布简化量化，减少噪声，常与PTQ结合。 | 改善低精度性能，尤其激活层；与Microsoft框架兼容。 | 引入预处理步骤；对层依赖敏感。 |
| **后训练量化 (PTQ) 变体** | Linear Quantization (Uniform Quantization) | 均匀线性映射浮点到整数，支持对称/非对称模式，作为基准方法。 | 简单高效；硬件支持广（如TensorRT）；易实现。 | 对范围极端值敏感，导致损失；需clipping优化。 |
| **后训练量化 (PTQ) 变体** | Sequential MSE Quantization | 逐层最小化均方误差（MSE）选择量化参数，支持动态调整。 | 精度保留好；适用于异构硬件；Qualcomm推广用于移动端。 | 序列过程慢；层间依赖复杂。 |
| **量化感知训练 (QAT) 相关** | Quantization-Aware Training (QAT) | 训练中插入伪量化节点，模拟低精度效果，使模型适应。 | 量化后精度更高；适合性能敏感应用；Hugging Face支持。 | 训练时间长，需要额外资源；不适用于已训模型。 |
| **量化感知训练 (QAT) 相关** | QLoRA (Quantized Low-Rank Adaptation) | 结合LoRA和量化（INT4+），低秩矩阵更新参数，用于微调。 | 极大降低微调内存（e.g., 消费GPU微调70B模型）；高性能保留。 | 限于微调，非从零训练；BitsandBytes库依赖。 |
| **权重仅量化 (WOQ) 及扩展** | Weight-Only Quantization (WOQ) | 仅量化权重，激活保持FP16等高精度。 | 减少存储需求，计算效率高；GPU/TPU友好；Intel工具支持。 | 激活仍需高内存；不完全优化激活。 |
| **非均匀/结构化量化** | Non-Uniform Quantization | 使用k-means聚类或码本映射，非线性量化。 | 处理不均分布好；压缩率高；适用于高维参数。 | 码本优化复杂；重建开销大。 |
| **非均匀/结构化量化** | Product Quantization (PQ) | 分解向量成子向量，用码本聚类量化，结构化压缩。 | 高效高维处理；支持分布式；向量DB集成好。 | 码本大小需调优；计算开销。 |
| **非均匀/结构化量化** | Blockwise Quantization | 将权重分成小块独立量化，处理范围差异。 | 适应参数变异；精度更好；常用于LLM块级优化。 | 块划分复杂；可能增加延迟。 |
| **非均匀/结构化量化** | ZeroQuant | 组级量化（group-wise），结合零点偏移，支持INT4以下。 | 处理分布不均；内存峰值低；Transformer兼容。 | 组大小调优难；少量噪声。 |
| **非均匀/结构化量化** | Sparse-Quantized Representation (SpQR) | 稀疏+量化，异常值高精度，其余低位压缩存储。 | 压缩率50%+；推理加速；针对LLM异常值。 | 需硬件支持稀疏；实现复杂。 |
| **高级/新兴方法** | Half-Quadratic Quantization (HQQ) | 无校准半二次优化量化，支持INT2/INT3。 | 无数据需求，速度快；性能接近QAT；Inferless推广。 | 对异常值敏感；可能需微调。 |
| **高级/新兴方法** | Attention-Aware Weight Quantization | 用Hessian迹度量注意力权重重要性，选择性量化。 | 提升注意力层性能；减少误差；arXiv研究支持。 | Hessian计算开销；限于PTQ。 |
| **高级/新兴方法** | FP8 Quantization | 使用8位浮点格式，平衡浮点精度和整数效率。 | 精度与效率平衡；硬件加速好；Reddit评估中常见。 | 硬件支持有限；不如INT低位压缩。 |
| **高级/新兴方法** | Binarization | 极端1位量化，权重为±1。 | 最大压缩；加速极致；研究用于特定场景。 | 精度损失大；不适合通用LLM。 
